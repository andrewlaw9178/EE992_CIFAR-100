{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "FUxPesQh3j12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUxPesQh3j12",
    "outputId": "4bb0c9b8-56f1-4bc9-d870-ed9a5c55cc8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 14 00:31:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0             48W /  400W |   12731MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "Ll6Dvqkk3lYe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ll6Dvqkk3lYe",
    "outputId": "f6cc3de1-dea6-4059-f97e-fdf5bd5de76d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789b266-7879-4d97-a26b-da3cb5a78005",
   "metadata": {
    "id": "4789b266-7879-4d97-a26b-da3cb5a78005"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cb342dd-fd78-418b-a94f-82f5361a1ae9",
   "metadata": {
    "id": "5cb342dd-fd78-418b-a94f-82f5361a1ae9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf # Access to models, datasets and training\n",
    "from tensorflow.keras.datasets import cifar100 # Access to CIFAR-100\n",
    "from tensorflow.keras.applications import ResNet50 # Access to pre-trained model\n",
    "from tensorflow.keras import layers, models, optimizers # Access to building blocks of a model\n",
    "from tensorflow import keras # Access to stuff for model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "from sklearn.model_selection import train_test_split # To help split dataset up\n",
    "from sklearn.metrics import confusion_matrix, classification_report # Analysis of model train, val & test\n",
    "import numpy as np # Manipulate data\n",
    "import pandas as pd # Statistical analysis of data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # Plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0fb22d-5f82-438e-98d1-49d77c1e052d",
   "metadata": {},
   "source": [
    "# With 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b00014-cb86-432f-8ebf-8953d5815c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat_2_times in range(2):\n",
    "    #### <<<<<<<<<<Load and process data>>>>>>>>>>\n",
    "    # Load CIFAR-100 dataset\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "    \n",
    "    # Split (8000) of training data into temporary set\n",
    "    X_temp, X_train, y_temp, y_train = train_test_split(X_train, y_train, test_size=0.84, stratify=y_train, random_state=42)\n",
    "    print(f\"X_temp.shape: {X_temp.shape}\\n\")\n",
    "    \n",
    "    # Split temp data into equal validation (4000) and testing (4000) data\n",
    "    X_temp_val, X_temp_test, y_temp_val, y_temp_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    print(f\"X_temp_val.shape: {X_temp_val.shape}\")\n",
    "    print(f\"y_temp_val.shape: {y_temp_val.shape}\")\n",
    "    print(f\"X_temp_test.shape: {X_temp_test.shape}\")\n",
    "    print(f\"y_temp_test.shape: {y_temp_test.shape}\\n\")\n",
    "    \n",
    "    # Split test data into validation (5000) and testing (5000)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "    \n",
    "    # Add temp_val to validation (9000) and temp_test to testing (9000) to get a 70/15/15 data split\n",
    "    X_val = np.concatenate((X_val, X_temp_val), axis=0)\n",
    "    y_val = np.concatenate((y_val, y_temp_val), axis=0)\n",
    "    X_test = np.concatenate((X_test, X_temp_test), axis=0)\n",
    "    y_test = np.concatenate((y_test, y_temp_test), axis=0)\n",
    "    \n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"y_train.shape: {y_train.shape}\")\n",
    "    print(f\"X_val.shape: {X_val.shape}\")\n",
    "    print(f\"y_val.shape: {y_val.shape}\")\n",
    "    print(f\"X_test.shape: {X_test.shape}\")\n",
    "    print(f\"y_test.shape: {y_test.shape}\\n\")\n",
    "    \n",
    "    def display_imgs(imgs, labels):\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            k = np.random.randint(0, imgs.shape[0])\n",
    "            if i == 0:\n",
    "                print(f\"labels[{k}].shape: {labels[k].shape}\")\n",
    "                print(f\"imgs[{k}].shape: {imgs[k].shape}\")\n",
    "            plt.imshow(imgs[k])\n",
    "            #plt.title(labels[k])\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    # Normalise images (scale to range [0, 1]) - Improves convergence speed & accuracy\n",
    "    X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    labels_names = ['beaver','dolphin','otter','seal','whale','aquarium fish','flatfish','ray','shark','trout',\n",
    "                   'orchids','poppies','roses','sunflowers','tulips','bottles','bowls','cans','cups','plates',\n",
    "                   'apples','mushrooms','oranges','pears','sweet peppers','clock','computer keyboard','lamp',\n",
    "                   'telephone','television','bed','chair','couch','table','wardrobe','bee','beetle','butterfly',\n",
    "                   'caterpillar','cockroach','bear','leopard','lion','tiger','wolf','bridge','castle','house',\n",
    "                   'road','skyscraper','cloud','forest','mountain','plain','sea','camel','cattle','chimpanzee',\n",
    "                   'elephant','kangaroo','fox','porcupine','possum','raccoon','skunk','crab','lobster','snail',\n",
    "                   'spider','worm','baby','boy','girl','man','woman','crocodile','dinosaur','lizard','snake',\n",
    "                   'turtle','hamster','mouse','rabbit','shrew','squirrel','maple','oak','palm','pine','willow',\n",
    "                   'bicycle','bus','motorcycle','pickup truck','train','lawn-mower','rocket','streetcar','tank',\n",
    "                   'tractor']\n",
    "    \n",
    "    def class_distrib(y, labels_names, dataset_name):\n",
    "        counts = pd.DataFrame(data=y).value_counts().sort_index()\n",
    "        #print(f\"counts:\\n{counts}\")\n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        ax.bar(labels_names, counts)\n",
    "        ax.set_xticklabels(labels_names, rotation=90, fontsize=15)\n",
    "        plt.title(f\"Distribution of '{dataset_name}' Dataset\", fontsize=25)\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    class_distrib(y_train, labels_names, \"Training\")\n",
    "    class_distrib(y_val, labels_names, \"Validating\")\n",
    "    class_distrib(y_test, labels_names, \"Testing\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (32, 32)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    val_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                   .map(lambda x, y: (tf.image.resize(x, (32, 32)),\n",
    "                                      tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                   .batch(batch_size)\n",
    "                   .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (32, 32)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                    .batch(batch_size)\n",
    "                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    print(f\"Training dataset:\\n {train_dataset}\")\n",
    "    for img, lbl in train_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nValidation dataset:\\n {val_dataset}\")\n",
    "    for img, lbl in val_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nTesting dataset:\\n {test_dataset}\")\n",
    "    for img, lbl in test_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    \n",
    "    \n",
    "    #### <<<<<<<<<<Pre-trained model>>>>>>>>>>\n",
    "    # Load ResNet50 pre-trained on ImageNet (w/out the top classification layer which is designed for ImageNet (diff dataset))\n",
    "    resnet_50_base = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "    \n",
    "    # Freeze the layers of VGG16 so they don't get updated during training - can unfreeze for fine tuning later\n",
    "    resnet_50_base.trainable = False\n",
    "    \n",
    "    # Add custom classification layers for CIFAR-100 (100 classes) - adapt model to CIFAR-100\n",
    "    model = models.Sequential([\n",
    "        resnet_50_base,\n",
    "        layers.GlobalAveragePooling2D(), # Better for ResNet than Flatten\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(100, activation='softmax')  # CIFAR-100 has 100 classes\n",
    "    ])\n",
    "    \n",
    "    for sample in test_dataset.take(1):\n",
    "        print(type(sample))  # Should be <class 'tuple'>\n",
    "        print(len(sample))  # Should be 2\n",
    "        print(type(sample[0]), type(sample[1]))  # Both should be <class 'tensorflow.Tensor'>\n",
    "        print(sample[0].shape)  # Should be (batch_size, 224, 224, 3)\n",
    "        print(sample[1].shape)  # Should be (batch_size, 100)\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    sample = next(iter(test_dataset.as_numpy_iterator()))\n",
    "    print(len(sample))  # Should be 2\n",
    "    print(type(sample[0]), type(sample[1]))  # Both should be <class 'numpy.ndarray'>\n",
    "    print(sample[0].shape, sample[1].shape)  # Should match model input and output\n",
    "    print(\"\\n\")\n",
    "    #for x, y in test_dataset.take(1):\n",
    "    #    print(type(x), type(y))  # Both should be <class 'tensorflow.Tensor'>\n",
    "    #for x_batch, y_batch in test_dataset.take(1):\n",
    "    #    test_loss, test_acc = model.evaluate(x_batch, y_batch)\n",
    "    #    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "    \n",
    "    # Compile the model\n",
    "    #tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', # or val_accuracy\n",
    "                                   patience=5, # Num. epochs with no improvements - help void overfitting\n",
    "                                   restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', # or val_accuracy\n",
    "    #                              factor=0.1, # Reduce lr by a factor\n",
    "    #                              patience=3, # Num epochs w/ no improvement\n",
    "    #                              min_lr=1e-6, # Min lr\n",
    "    #                              verbose=1)\n",
    "    #tensorboard = TensorBoard(log_dir='./logs', # Logs directory\n",
    "    #                         histogram_freq=1, # Logs histograms for weights/activations\n",
    "    #                         write_graph=True, # Logs graph of model\n",
    "    #                         write_images=True) # Log images like weight histogram\n",
    "    #checkpoint = ModelCheckpoint('best_model.h5',\n",
    "    #                             monitor='val_loss', # or val_accuracy\n",
    "    #                             save_best_only=True, # Save only best model\n",
    "    #                             mode='min', # min for loss or max for accuracy\n",
    "    #                             verbose=1)\n",
    "    #cvs_logger = CSVLogger('training_log.csv', seperator=',', append=True) # Save train metrics to analyse\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=25,\n",
    "                        batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history.history['loss'])+1)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    def plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc):\n",
    "        # Plot Training and Validation Loss\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_loss, label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_acc, label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (32, 32))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    #tensorboard --logdir==path_to_your_logs\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist\n",
    "    \n",
    "    #### <<<<<<<<<<Fine-Tune>>>>>>>>>>\n",
    "    #### <<<<<<<<<<Adapt Model>>>>>>>>>>\n",
    "    # Unfreeze last 10 layers\n",
    "    for layer in resnet_50_base.layers[-10:]:\n",
    "        layer.trainable = True # Allow layers to be updated\n",
    "    \n",
    "    # Compile again w/ lower learning rate (prevents destroying learned features)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Modify Dataset>>>>>>>>>>\n",
    "    def augment_dataset(x, y):\n",
    "        x = tf.image.resize(x, (32, 32))  # Resize images\n",
    "        x = tf.image.random_flip_left_right(x)  # Random horizontal flip\n",
    "        x = tf.image.random_brightness(x, max_delta=0.2)  # Adjust brightness\n",
    "        x = tf.image.random_contrast(x, lower=0.8, upper=1.2)  # Adjust contrast\n",
    "        y = tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))  # One-hot encode labels\n",
    "        return x, y\n",
    "    train_dataset_aug = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(augment_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    # Not val or test as augment train helps generalise better, but want to provide consistent benchmark for eval perf\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Train the model\n",
    "    history_fine_tune = model.fit(train_dataset, validation_data=val_dataset, epochs=15,\n",
    "                                  batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history_fine_tune.history['loss'])+1)\n",
    "    train_loss = history_fine_tune.history['loss']\n",
    "    val_loss = history_fine_tune.history['val_loss']\n",
    "    train_acc = history_fine_tune.history['accuracy']\n",
    "    val_acc = history_fine_tune.history['val_accuracy']\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history_fine_tune.history)\n",
    "    hist['epoch'] = history_fine_tune.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95291bd-f637-4499-8e77-51cb6fc6d643",
   "metadata": {
    "id": "b95291bd-f637-4499-8e77-51cb6fc6d643"
   },
   "source": [
    "# With 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8d37b3-de7f-43c2-93c5-aca3b3411be1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec8d37b3-de7f-43c2-93c5-aca3b3411be1",
    "outputId": "cc7b3a37-e813-4268-feea-f0824edd4778"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3281157838.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    for repeat_2_times:\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for repeat_2_times in range(2):\n",
    "    #### <<<<<<<<<<Load and process data>>>>>>>>>>\n",
    "    # Load CIFAR-100 dataset\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "    \n",
    "    # Split (8000) of training data into temporary set\n",
    "    X_temp, X_train, y_temp, y_train = train_test_split(X_train, y_train, test_size=0.84, stratify=y_train, random_state=42)\n",
    "    print(f\"X_temp.shape: {X_temp.shape}\\n\")\n",
    "    \n",
    "    # Split temp data into equal validation (4000) and testing (4000) data\n",
    "    X_temp_val, X_temp_test, y_temp_val, y_temp_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    print(f\"X_temp_val.shape: {X_temp_val.shape}\")\n",
    "    print(f\"y_temp_val.shape: {y_temp_val.shape}\")\n",
    "    print(f\"X_temp_test.shape: {X_temp_test.shape}\")\n",
    "    print(f\"y_temp_test.shape: {y_temp_test.shape}\\n\")\n",
    "    \n",
    "    # Split test data into validation (5000) and testing (5000)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "    \n",
    "    # Add temp_val to validation (9000) and temp_test to testing (9000) to get a 70/15/15 data split\n",
    "    X_val = np.concatenate((X_val, X_temp_val), axis=0)\n",
    "    y_val = np.concatenate((y_val, y_temp_val), axis=0)\n",
    "    X_test = np.concatenate((X_test, X_temp_test), axis=0)\n",
    "    y_test = np.concatenate((y_test, y_temp_test), axis=0)\n",
    "    \n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"y_train.shape: {y_train.shape}\")\n",
    "    print(f\"X_val.shape: {X_val.shape}\")\n",
    "    print(f\"y_val.shape: {y_val.shape}\")\n",
    "    print(f\"X_test.shape: {X_test.shape}\")\n",
    "    print(f\"y_test.shape: {y_test.shape}\\n\")\n",
    "    \n",
    "    def display_imgs(imgs, labels):\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            k = np.random.randint(0, imgs.shape[0])\n",
    "            if i == 0:\n",
    "                print(f\"labels[{k}].shape: {labels[k].shape}\")\n",
    "                print(f\"imgs[{k}].shape: {imgs[k].shape}\")\n",
    "            plt.imshow(imgs[k])\n",
    "            #plt.title(labels[k])\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    # Normalise images (scale to range [0, 1]) - Improves convergence speed & accuracy\n",
    "    X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    labels_names = ['beaver','dolphin','otter','seal','whale','aquarium fish','flatfish','ray','shark','trout',\n",
    "                   'orchids','poppies','roses','sunflowers','tulips','bottles','bowls','cans','cups','plates',\n",
    "                   'apples','mushrooms','oranges','pears','sweet peppers','clock','computer keyboard','lamp',\n",
    "                   'telephone','television','bed','chair','couch','table','wardrobe','bee','beetle','butterfly',\n",
    "                   'caterpillar','cockroach','bear','leopard','lion','tiger','wolf','bridge','castle','house',\n",
    "                   'road','skyscraper','cloud','forest','mountain','plain','sea','camel','cattle','chimpanzee',\n",
    "                   'elephant','kangaroo','fox','porcupine','possum','raccoon','skunk','crab','lobster','snail',\n",
    "                   'spider','worm','baby','boy','girl','man','woman','crocodile','dinosaur','lizard','snake',\n",
    "                   'turtle','hamster','mouse','rabbit','shrew','squirrel','maple','oak','palm','pine','willow',\n",
    "                   'bicycle','bus','motorcycle','pickup truck','train','lawn-mower','rocket','streetcar','tank',\n",
    "                   'tractor']\n",
    "    \n",
    "    def class_distrib(y, labels_names, dataset_name):\n",
    "        counts = pd.DataFrame(data=y).value_counts().sort_index()\n",
    "        #print(f\"counts:\\n{counts}\")\n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        ax.bar(labels_names, counts)\n",
    "        ax.set_xticklabels(labels_names, rotation=90, fontsize=15)\n",
    "        plt.title(f\"Distribution of '{dataset_name}' Dataset\", fontsize=25)\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    class_distrib(y_train, labels_names, \"Training\")\n",
    "    class_distrib(y_val, labels_names, \"Validating\")\n",
    "    class_distrib(y_test, labels_names, \"Testing\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (128, 128)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    val_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                   .map(lambda x, y: (tf.image.resize(x, (128, 128)),\n",
    "                                      tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                   .batch(batch_size)\n",
    "                   .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (128, 128)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                    .batch(batch_size)\n",
    "                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    print(f\"Training dataset:\\n {train_dataset}\")\n",
    "    for img, lbl in train_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nValidation dataset:\\n {val_dataset}\")\n",
    "    for img, lbl in val_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nTesting dataset:\\n {test_dataset}\")\n",
    "    for img, lbl in test_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    \n",
    "    \n",
    "    #### <<<<<<<<<<Pre-trained model>>>>>>>>>>\n",
    "    # Load ResNet50 pre-trained on ImageNet (w/out the top classification layer which is designed for ImageNet (diff dataset))\n",
    "    resnet_50_base = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    \n",
    "    # Freeze the layers of VGG16 so they don't get updated during training - can unfreeze for fine tuning later\n",
    "    resnet_50_base.trainable = False\n",
    "    \n",
    "    # Add custom classification layers for CIFAR-100 (100 classes) - adapt model to CIFAR-100\n",
    "    model = models.Sequential([\n",
    "        resnet_50_base,\n",
    "        layers.GlobalAveragePooling2D(), # Better for ResNet than Flatten\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(100, activation='softmax')  # CIFAR-100 has 100 classes\n",
    "    ])\n",
    "    \n",
    "    for sample in test_dataset.take(1):\n",
    "        print(type(sample))  # Should be <class 'tuple'>\n",
    "        print(len(sample))  # Should be 2\n",
    "        print(type(sample[0]), type(sample[1]))  # Both should be <class 'tensorflow.Tensor'>\n",
    "        print(sample[0].shape)  # Should be (batch_size, 224, 224, 3)\n",
    "        print(sample[1].shape)  # Should be (batch_size, 100)\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    sample = next(iter(test_dataset.as_numpy_iterator()))\n",
    "    print(len(sample))  # Should be 2\n",
    "    print(type(sample[0]), type(sample[1]))  # Both should be <class 'numpy.ndarray'>\n",
    "    print(sample[0].shape, sample[1].shape)  # Should match model input and output\n",
    "    print(\"\\n\")\n",
    "    #for x, y in test_dataset.take(1):\n",
    "    #    print(type(x), type(y))  # Both should be <class 'tensorflow.Tensor'>\n",
    "    #for x_batch, y_batch in test_dataset.take(1):\n",
    "    #    test_loss, test_acc = model.evaluate(x_batch, y_batch)\n",
    "    #    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "    \n",
    "    # Compile the model\n",
    "    #tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', # or val_accuracy\n",
    "                                   patience=5, # Num. epochs with no improvements - help void overfitting\n",
    "                                   restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', # or val_accuracy\n",
    "    #                              factor=0.1, # Reduce lr by a factor\n",
    "    #                              patience=3, # Num epochs w/ no improvement\n",
    "    #                              min_lr=1e-6, # Min lr\n",
    "    #                              verbose=1)\n",
    "    #tensorboard = TensorBoard(log_dir='./logs', # Logs directory\n",
    "    #                         histogram_freq=1, # Logs histograms for weights/activations\n",
    "    #                         write_graph=True, # Logs graph of model\n",
    "    #                         write_images=True) # Log images like weight histogram\n",
    "    #checkpoint = ModelCheckpoint('best_model.h5',\n",
    "    #                             monitor='val_loss', # or val_accuracy\n",
    "    #                             save_best_only=True, # Save only best model\n",
    "    #                             mode='min', # min for loss or max for accuracy\n",
    "    #                             verbose=1)\n",
    "    #cvs_logger = CSVLogger('training_log.csv', seperator=',', append=True) # Save train metrics to analyse\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=25,\n",
    "                        batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history.history['loss'])+1)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    def plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc):\n",
    "        # Plot Training and Validation Loss\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_loss, label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_acc, label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (128, 128))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    #tensorboard --logdir==path_to_your_logs\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist\n",
    "    \n",
    "    #### <<<<<<<<<<Fine-Tune>>>>>>>>>>\n",
    "    #### <<<<<<<<<<Adapt Model>>>>>>>>>>\n",
    "    # Unfreeze last 10 layers\n",
    "    for layer in resnet_50_base.layers[-10:]:\n",
    "        layer.trainable = True # Allow layers to be updated\n",
    "    \n",
    "    # Compile again w/ lower learning rate (prevents destroying learned features)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Modify Dataset>>>>>>>>>>\n",
    "    def augment_dataset(x, y):\n",
    "        x = tf.image.resize(x, (128, 128))  # Resize images\n",
    "        x = tf.image.random_flip_left_right(x)  # Random horizontal flip\n",
    "        x = tf.image.random_brightness(x, max_delta=0.2)  # Adjust brightness\n",
    "        x = tf.image.random_contrast(x, lower=0.8, upper=1.2)  # Adjust contrast\n",
    "        y = tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))  # One-hot encode labels\n",
    "        return x, y\n",
    "    train_dataset_aug = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(augment_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    # Not val or test as augment train helps generalise better, but want to provide consistent benchmark for eval perf\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Train the model\n",
    "    history_fine_tune = model.fit(train_dataset, validation_data=val_dataset, epochs=15,\n",
    "                                  batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history_fine_tune.history['loss'])+1)\n",
    "    train_loss = history_fine_tune.history['loss']\n",
    "    val_loss = history_fine_tune.history['val_loss']\n",
    "    train_acc = history_fine_tune.history['accuracy']\n",
    "    val_acc = history_fine_tune.history['val_accuracy']\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history_fine_tune.history)\n",
    "    hist['epoch'] = history_fine_tune.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1121697-2889-4556-a7de-02dc61cbc5c1",
   "metadata": {},
   "source": [
    "# With 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ba23c-caed-48c2-9eb7-bdd69be8096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat_2_times in range(2):\n",
    "    #### <<<<<<<<<<Load and process data>>>>>>>>>>\n",
    "    # Load CIFAR-100 dataset\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "    \n",
    "    # Split (8000) of training data into temporary set\n",
    "    X_temp, X_train, y_temp, y_train = train_test_split(X_train, y_train, test_size=0.84, stratify=y_train, random_state=42)\n",
    "    print(f\"X_temp.shape: {X_temp.shape}\\n\")\n",
    "    \n",
    "    # Split temp data into equal validation (4000) and testing (4000) data\n",
    "    X_temp_val, X_temp_test, y_temp_val, y_temp_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    print(f\"X_temp_val.shape: {X_temp_val.shape}\")\n",
    "    print(f\"y_temp_val.shape: {y_temp_val.shape}\")\n",
    "    print(f\"X_temp_test.shape: {X_temp_test.shape}\")\n",
    "    print(f\"y_temp_test.shape: {y_temp_test.shape}\\n\")\n",
    "    \n",
    "    # Split test data into validation (5000) and testing (5000)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "    \n",
    "    # Add temp_val to validation (9000) and temp_test to testing (9000) to get a 70/15/15 data split\n",
    "    X_val = np.concatenate((X_val, X_temp_val), axis=0)\n",
    "    y_val = np.concatenate((y_val, y_temp_val), axis=0)\n",
    "    X_test = np.concatenate((X_test, X_temp_test), axis=0)\n",
    "    y_test = np.concatenate((y_test, y_temp_test), axis=0)\n",
    "    \n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"y_train.shape: {y_train.shape}\")\n",
    "    print(f\"X_val.shape: {X_val.shape}\")\n",
    "    print(f\"y_val.shape: {y_val.shape}\")\n",
    "    print(f\"X_test.shape: {X_test.shape}\")\n",
    "    print(f\"y_test.shape: {y_test.shape}\\n\")\n",
    "    \n",
    "    def display_imgs(imgs, labels):\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            k = np.random.randint(0, imgs.shape[0])\n",
    "            if i == 0:\n",
    "                print(f\"labels[{k}].shape: {labels[k].shape}\")\n",
    "                print(f\"imgs[{k}].shape: {imgs[k].shape}\")\n",
    "            plt.imshow(imgs[k])\n",
    "            #plt.title(labels[k])\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    # Normalise images (scale to range [0, 1]) - Improves convergence speed & accuracy\n",
    "    X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    labels_names = ['beaver','dolphin','otter','seal','whale','aquarium fish','flatfish','ray','shark','trout',\n",
    "                   'orchids','poppies','roses','sunflowers','tulips','bottles','bowls','cans','cups','plates',\n",
    "                   'apples','mushrooms','oranges','pears','sweet peppers','clock','computer keyboard','lamp',\n",
    "                   'telephone','television','bed','chair','couch','table','wardrobe','bee','beetle','butterfly',\n",
    "                   'caterpillar','cockroach','bear','leopard','lion','tiger','wolf','bridge','castle','house',\n",
    "                   'road','skyscraper','cloud','forest','mountain','plain','sea','camel','cattle','chimpanzee',\n",
    "                   'elephant','kangaroo','fox','porcupine','possum','raccoon','skunk','crab','lobster','snail',\n",
    "                   'spider','worm','baby','boy','girl','man','woman','crocodile','dinosaur','lizard','snake',\n",
    "                   'turtle','hamster','mouse','rabbit','shrew','squirrel','maple','oak','palm','pine','willow',\n",
    "                   'bicycle','bus','motorcycle','pickup truck','train','lawn-mower','rocket','streetcar','tank',\n",
    "                   'tractor']\n",
    "    \n",
    "    def class_distrib(y, labels_names, dataset_name):\n",
    "        counts = pd.DataFrame(data=y).value_counts().sort_index()\n",
    "        #print(f\"counts:\\n{counts}\")\n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        ax.bar(labels_names, counts)\n",
    "        ax.set_xticklabels(labels_names, rotation=90, fontsize=15)\n",
    "        plt.title(f\"Distribution of '{dataset_name}' Dataset\", fontsize=25)\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    class_distrib(y_train, labels_names, \"Training\")\n",
    "    class_distrib(y_val, labels_names, \"Validating\")\n",
    "    class_distrib(y_test, labels_names, \"Testing\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    val_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                   .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                      tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                   .batch(batch_size)\n",
    "                   .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                    .batch(batch_size)\n",
    "                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    print(f\"Training dataset:\\n {train_dataset}\")\n",
    "    for img, lbl in train_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nValidation dataset:\\n {val_dataset}\")\n",
    "    for img, lbl in val_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nTesting dataset:\\n {test_dataset}\")\n",
    "    for img, lbl in test_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    \n",
    "    \n",
    "    #### <<<<<<<<<<Pre-trained model>>>>>>>>>>\n",
    "    # Load ResNet50 pre-trained on ImageNet (w/out the top classification layer which is designed for ImageNet (diff dataset))\n",
    "    resnet_50_base = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Freeze the layers of VGG16 so they don't get updated during training - can unfreeze for fine tuning later\n",
    "    resnet_50_base.trainable = False\n",
    "    \n",
    "    # Add custom classification layers for CIFAR-100 (100 classes) - adapt model to CIFAR-100\n",
    "    model = models.Sequential([\n",
    "        resnet_50_base,\n",
    "        layers.GlobalAveragePooling2D(), # Better for ResNet than Flatten\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(100, activation='softmax')  # CIFAR-100 has 100 classes\n",
    "    ])\n",
    "    \n",
    "    for sample in test_dataset.take(1):\n",
    "        print(type(sample))  # Should be <class 'tuple'>\n",
    "        print(len(sample))  # Should be 2\n",
    "        print(type(sample[0]), type(sample[1]))  # Both should be <class 'tensorflow.Tensor'>\n",
    "        print(sample[0].shape)  # Should be (batch_size, 224, 224, 3)\n",
    "        print(sample[1].shape)  # Should be (batch_size, 100)\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    sample = next(iter(test_dataset.as_numpy_iterator()))\n",
    "    print(len(sample))  # Should be 2\n",
    "    print(type(sample[0]), type(sample[1]))  # Both should be <class 'numpy.ndarray'>\n",
    "    print(sample[0].shape, sample[1].shape)  # Should match model input and output\n",
    "    print(\"\\n\")\n",
    "    #for x, y in test_dataset.take(1):\n",
    "    #    print(type(x), type(y))  # Both should be <class 'tensorflow.Tensor'>\n",
    "    #for x_batch, y_batch in test_dataset.take(1):\n",
    "    #    test_loss, test_acc = model.evaluate(x_batch, y_batch)\n",
    "    #    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "    \n",
    "    # Compile the model\n",
    "    #tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', # or val_accuracy\n",
    "                                   patience=5, # Num. epochs with no improvements - help void overfitting\n",
    "                                   restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', # or val_accuracy\n",
    "    #                              factor=0.1, # Reduce lr by a factor\n",
    "    #                              patience=3, # Num epochs w/ no improvement\n",
    "    #                              min_lr=1e-6, # Min lr\n",
    "    #                              verbose=1)\n",
    "    #tensorboard = TensorBoard(log_dir='./logs', # Logs directory\n",
    "    #                         histogram_freq=1, # Logs histograms for weights/activations\n",
    "    #                         write_graph=True, # Logs graph of model\n",
    "    #                         write_images=True) # Log images like weight histogram\n",
    "    #checkpoint = ModelCheckpoint('best_model.h5',\n",
    "    #                             monitor='val_loss', # or val_accuracy\n",
    "    #                             save_best_only=True, # Save only best model\n",
    "    #                             mode='min', # min for loss or max for accuracy\n",
    "    #                             verbose=1)\n",
    "    #cvs_logger = CSVLogger('training_log.csv', seperator=',', append=True) # Save train metrics to analyse\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=25,\n",
    "                        batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history.history['loss'])+1)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    def plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc):\n",
    "        # Plot Training and Validation Loss\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_loss, label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_acc, label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    #tensorboard --logdir==path_to_your_logs\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist\n",
    "    \n",
    "    #### <<<<<<<<<<Fine-Tune>>>>>>>>>>\n",
    "    #### <<<<<<<<<<Adapt Model>>>>>>>>>>\n",
    "    # Unfreeze last 10 layers\n",
    "    for layer in resnet_50_base.layers[-10:]:\n",
    "        layer.trainable = True # Allow layers to be updated\n",
    "    \n",
    "    # Compile again w/ lower learning rate (prevents destroying learned features)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Modify Dataset>>>>>>>>>>\n",
    "    def augment_dataset(x, y):\n",
    "        x = tf.image.resize(x, (224, 224))  # Resize images\n",
    "        x = tf.image.random_flip_left_right(x)  # Random horizontal flip\n",
    "        x = tf.image.random_brightness(x, max_delta=0.2)  # Adjust brightness\n",
    "        x = tf.image.random_contrast(x, lower=0.8, upper=1.2)  # Adjust contrast\n",
    "        y = tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))  # One-hot encode labels\n",
    "        return x, y\n",
    "    train_dataset_aug = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(augment_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    # Not val or test as augment train helps generalise better, but want to provide consistent benchmark for eval perf\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Train the model\n",
    "    history_fine_tune = model.fit(train_dataset, validation_data=val_dataset, epochs=15,\n",
    "                                  batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history_fine_tune.history['loss'])+1)\n",
    "    train_loss = history_fine_tune.history['loss']\n",
    "    val_loss = history_fine_tune.history['val_loss']\n",
    "    train_acc = history_fine_tune.history['accuracy']\n",
    "    val_acc = history_fine_tune.history['val_accuracy']\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history_fine_tune.history)\n",
    "    hist['epoch'] = history_fine_tune.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1dcaa-d39f-43a4-9016-c3c57fffd0f8",
   "metadata": {},
   "source": [
    "# Different mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3f2a3-a300-4637-a028-af26bc524952",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat_2_times in range(2):\n",
    "    #### <<<<<<<<<<Load and process data>>>>>>>>>>\n",
    "    # Load CIFAR-100 dataset\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "    \n",
    "    # Split (8000) of training data into temporary set\n",
    "    X_temp, X_train, y_temp, y_train = train_test_split(X_train, y_train, test_size=0.84, stratify=y_train, random_state=42)\n",
    "    print(f\"X_temp.shape: {X_temp.shape}\\n\")\n",
    "    \n",
    "    # Split temp data into equal validation (4000) and testing (4000) data\n",
    "    X_temp_val, X_temp_test, y_temp_val, y_temp_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    print(f\"X_temp_val.shape: {X_temp_val.shape}\")\n",
    "    print(f\"y_temp_val.shape: {y_temp_val.shape}\")\n",
    "    print(f\"X_temp_test.shape: {X_temp_test.shape}\")\n",
    "    print(f\"y_temp_test.shape: {y_temp_test.shape}\\n\")\n",
    "    \n",
    "    # Split test data into validation (5000) and testing (5000)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "    \n",
    "    # Add temp_val to validation (9000) and temp_test to testing (9000) to get a 70/15/15 data split\n",
    "    X_val = np.concatenate((X_val, X_temp_val), axis=0)\n",
    "    y_val = np.concatenate((y_val, y_temp_val), axis=0)\n",
    "    X_test = np.concatenate((X_test, X_temp_test), axis=0)\n",
    "    y_test = np.concatenate((y_test, y_temp_test), axis=0)\n",
    "    \n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"y_train.shape: {y_train.shape}\")\n",
    "    print(f\"X_val.shape: {X_val.shape}\")\n",
    "    print(f\"y_val.shape: {y_val.shape}\")\n",
    "    print(f\"X_test.shape: {X_test.shape}\")\n",
    "    print(f\"y_test.shape: {y_test.shape}\\n\")\n",
    "    \n",
    "    def display_imgs(imgs, labels):\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            k = np.random.randint(0, imgs.shape[0])\n",
    "            if i == 0:\n",
    "                print(f\"labels[{k}].shape: {labels[k].shape}\")\n",
    "                print(f\"imgs[{k}].shape: {imgs[k].shape}\")\n",
    "            plt.imshow(imgs[k])\n",
    "            #plt.title(labels[k])\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    # Normalise images (scale to range [0, 1]) - Improves convergence speed & accuracy\n",
    "    X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    labels_names = ['beaver','dolphin','otter','seal','whale','aquarium fish','flatfish','ray','shark','trout',\n",
    "                   'orchids','poppies','roses','sunflowers','tulips','bottles','bowls','cans','cups','plates',\n",
    "                   'apples','mushrooms','oranges','pears','sweet peppers','clock','computer keyboard','lamp',\n",
    "                   'telephone','television','bed','chair','couch','table','wardrobe','bee','beetle','butterfly',\n",
    "                   'caterpillar','cockroach','bear','leopard','lion','tiger','wolf','bridge','castle','house',\n",
    "                   'road','skyscraper','cloud','forest','mountain','plain','sea','camel','cattle','chimpanzee',\n",
    "                   'elephant','kangaroo','fox','porcupine','possum','raccoon','skunk','crab','lobster','snail',\n",
    "                   'spider','worm','baby','boy','girl','man','woman','crocodile','dinosaur','lizard','snake',\n",
    "                   'turtle','hamster','mouse','rabbit','shrew','squirrel','maple','oak','palm','pine','willow',\n",
    "                   'bicycle','bus','motorcycle','pickup truck','train','lawn-mower','rocket','streetcar','tank',\n",
    "                   'tractor']\n",
    "    \n",
    "    def class_distrib(y, labels_names, dataset_name):\n",
    "        counts = pd.DataFrame(data=y).value_counts().sort_index()\n",
    "        #print(f\"counts:\\n{counts}\")\n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        ax.bar(labels_names, counts)\n",
    "        ax.set_xticklabels(labels_names, rotation=90, fontsize=15)\n",
    "        plt.title(f\"Distribution of '{dataset_name}' Dataset\", fontsize=25)\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    class_distrib(y_train, labels_names, \"Training\")\n",
    "    class_distrib(y_val, labels_names, \"Validating\")\n",
    "    class_distrib(y_test, labels_names, \"Testing\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    val_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                   .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                      tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                   .batch(batch_size)\n",
    "                   .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                    .batch(batch_size)\n",
    "                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    print(f\"Training dataset:\\n {train_dataset}\")\n",
    "    for img, lbl in train_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nValidation dataset:\\n {val_dataset}\")\n",
    "    for img, lbl in val_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nTesting dataset:\\n {test_dataset}\")\n",
    "    for img, lbl in test_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    \n",
    "    \n",
    "    #### <<<<<<<<<<Pre-trained model>>>>>>>>>>\n",
    "    # Load ResNet50 pre-trained on ImageNet (w/out the top classification layer which is designed for ImageNet (diff dataset))\n",
    "    resnet_50_base = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Freeze the layers of VGG16 so they don't get updated during training - can unfreeze for fine tuning later\n",
    "    resnet_50_base.trainable = False\n",
    "    \n",
    "    # Add custom classification layers for CIFAR-100 (100 classes) - adapt model to CIFAR-100\n",
    "    model = models.Sequential([\n",
    "        resnet_50_base,\n",
    "        layers.GlobalAveragePooling2D(), # Better for ResNet than Flatten\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(100, activation='softmax')  # CIFAR-100 has 100 classes\n",
    "    ])\n",
    "    \n",
    "    for sample in test_dataset.take(1):\n",
    "        print(type(sample))  # Should be <class 'tuple'>\n",
    "        print(len(sample))  # Should be 2\n",
    "        print(type(sample[0]), type(sample[1]))  # Both should be <class 'tensorflow.Tensor'>\n",
    "        print(sample[0].shape)  # Should be (batch_size, 224, 224, 3)\n",
    "        print(sample[1].shape)  # Should be (batch_size, 100)\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    sample = next(iter(test_dataset.as_numpy_iterator()))\n",
    "    print(len(sample))  # Should be 2\n",
    "    print(type(sample[0]), type(sample[1]))  # Both should be <class 'numpy.ndarray'>\n",
    "    print(sample[0].shape, sample[1].shape)  # Should match model input and output\n",
    "    print(\"\\n\")\n",
    "    #for x, y in test_dataset.take(1):\n",
    "    #    print(type(x), type(y))  # Both should be <class 'tensorflow.Tensor'>\n",
    "    #for x_batch, y_batch in test_dataset.take(1):\n",
    "    #    test_loss, test_acc = model.evaluate(x_batch, y_batch)\n",
    "    #    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "    \n",
    "    # Compile the model\n",
    "    #tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.003),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', # or val_accuracy\n",
    "                                   patience=5, # Num. epochs with no improvements - help void overfitting\n",
    "                                   restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', # or val_accuracy\n",
    "    #                              factor=0.1, # Reduce lr by a factor\n",
    "    #                              patience=3, # Num epochs w/ no improvement\n",
    "    #                              min_lr=1e-6, # Min lr\n",
    "    #                              verbose=1)\n",
    "    #tensorboard = TensorBoard(log_dir='./logs', # Logs directory\n",
    "    #                         histogram_freq=1, # Logs histograms for weights/activations\n",
    "    #                         write_graph=True, # Logs graph of model\n",
    "    #                         write_images=True) # Log images like weight histogram\n",
    "    #checkpoint = ModelCheckpoint('best_model.h5',\n",
    "    #                             monitor='val_loss', # or val_accuracy\n",
    "    #                             save_best_only=True, # Save only best model\n",
    "    #                             mode='min', # min for loss or max for accuracy\n",
    "    #                             verbose=1)\n",
    "    #cvs_logger = CSVLogger('training_log.csv', seperator=',', append=True) # Save train metrics to analyse\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=25,\n",
    "                        batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history.history['loss'])+1)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    def plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc):\n",
    "        # Plot Training and Validation Loss\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_loss, label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_acc, label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    #tensorboard --logdir==path_to_your_logs\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist\n",
    "    \n",
    "    #### <<<<<<<<<<Fine-Tune>>>>>>>>>>\n",
    "    #### <<<<<<<<<<Adapt Model>>>>>>>>>>\n",
    "    # Unfreeze last 10 layers\n",
    "    for layer in resnet_50_base.layers[-10:]:\n",
    "        layer.trainable = True # Allow layers to be updated\n",
    "    \n",
    "    # Compile again w/ lower learning rate (prevents destroying learned features)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Modify Dataset>>>>>>>>>>\n",
    "    def augment_dataset(x, y):\n",
    "        x = tf.image.resize(x, (224, 224))  # Resize images\n",
    "        x = tf.image.random_flip_left_right(x)  # Random horizontal flip\n",
    "        x = tf.image.random_brightness(x, max_delta=0.2)  # Adjust brightness\n",
    "        x = tf.image.random_contrast(x, lower=0.8, upper=1.2)  # Adjust contrast\n",
    "        y = tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))  # One-hot encode labels\n",
    "        return x, y\n",
    "    train_dataset_aug = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(augment_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    # Not val or test as augment train helps generalise better, but want to provide consistent benchmark for eval perf\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Train the model\n",
    "    history_fine_tune = model.fit(train_dataset, validation_data=val_dataset, epochs=15,\n",
    "                                  batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history_fine_tune.history['loss'])+1)\n",
    "    train_loss = history_fine_tune.history['loss']\n",
    "    val_loss = history_fine_tune.history['val_loss']\n",
    "    train_acc = history_fine_tune.history['accuracy']\n",
    "    val_acc = history_fine_tune.history['val_accuracy']\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history_fine_tune.history)\n",
    "    hist['epoch'] = history_fine_tune.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6549b-69c2-4c56-ae54-f88eb3bde0c8",
   "metadata": {},
   "source": [
    "# Unfreeze all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442a48e-7c42-4bed-be6b-eb2f5a2c6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat_2_times in range(2):\n",
    "    #### <<<<<<<<<<Load and process data>>>>>>>>>>\n",
    "    # Load CIFAR-100 dataset\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "    \n",
    "    # Split (8000) of training data into temporary set\n",
    "    X_temp, X_train, y_temp, y_train = train_test_split(X_train, y_train, test_size=0.84, stratify=y_train, random_state=42)\n",
    "    print(f\"X_temp.shape: {X_temp.shape}\\n\")\n",
    "    \n",
    "    # Split temp data into equal validation (4000) and testing (4000) data\n",
    "    X_temp_val, X_temp_test, y_temp_val, y_temp_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    print(f\"X_temp_val.shape: {X_temp_val.shape}\")\n",
    "    print(f\"y_temp_val.shape: {y_temp_val.shape}\")\n",
    "    print(f\"X_temp_test.shape: {X_temp_test.shape}\")\n",
    "    print(f\"y_temp_test.shape: {y_temp_test.shape}\\n\")\n",
    "    \n",
    "    # Split test data into validation (5000) and testing (5000)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "    \n",
    "    # Add temp_val to validation (9000) and temp_test to testing (9000) to get a 70/15/15 data split\n",
    "    X_val = np.concatenate((X_val, X_temp_val), axis=0)\n",
    "    y_val = np.concatenate((y_val, y_temp_val), axis=0)\n",
    "    X_test = np.concatenate((X_test, X_temp_test), axis=0)\n",
    "    y_test = np.concatenate((y_test, y_temp_test), axis=0)\n",
    "    \n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"y_train.shape: {y_train.shape}\")\n",
    "    print(f\"X_val.shape: {X_val.shape}\")\n",
    "    print(f\"y_val.shape: {y_val.shape}\")\n",
    "    print(f\"X_test.shape: {X_test.shape}\")\n",
    "    print(f\"y_test.shape: {y_test.shape}\\n\")\n",
    "    \n",
    "    def display_imgs(imgs, labels):\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            k = np.random.randint(0, imgs.shape[0])\n",
    "            if i == 0:\n",
    "                print(f\"labels[{k}].shape: {labels[k].shape}\")\n",
    "                print(f\"imgs[{k}].shape: {imgs[k].shape}\")\n",
    "            plt.imshow(imgs[k])\n",
    "            #plt.title(labels[k])\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    # Normalise images (scale to range [0, 1]) - Improves convergence speed & accuracy\n",
    "    X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    labels_names = ['beaver','dolphin','otter','seal','whale','aquarium fish','flatfish','ray','shark','trout',\n",
    "                   'orchids','poppies','roses','sunflowers','tulips','bottles','bowls','cans','cups','plates',\n",
    "                   'apples','mushrooms','oranges','pears','sweet peppers','clock','computer keyboard','lamp',\n",
    "                   'telephone','television','bed','chair','couch','table','wardrobe','bee','beetle','butterfly',\n",
    "                   'caterpillar','cockroach','bear','leopard','lion','tiger','wolf','bridge','castle','house',\n",
    "                   'road','skyscraper','cloud','forest','mountain','plain','sea','camel','cattle','chimpanzee',\n",
    "                   'elephant','kangaroo','fox','porcupine','possum','raccoon','skunk','crab','lobster','snail',\n",
    "                   'spider','worm','baby','boy','girl','man','woman','crocodile','dinosaur','lizard','snake',\n",
    "                   'turtle','hamster','mouse','rabbit','shrew','squirrel','maple','oak','palm','pine','willow',\n",
    "                   'bicycle','bus','motorcycle','pickup truck','train','lawn-mower','rocket','streetcar','tank',\n",
    "                   'tractor']\n",
    "    \n",
    "    def class_distrib(y, labels_names, dataset_name):\n",
    "        counts = pd.DataFrame(data=y).value_counts().sort_index()\n",
    "        #print(f\"counts:\\n{counts}\")\n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        ax.bar(labels_names, counts)\n",
    "        ax.set_xticklabels(labels_names, rotation=90, fontsize=15)\n",
    "        plt.title(f\"Distribution of '{dataset_name}' Dataset\", fontsize=25)\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    class_distrib(y_train, labels_names, \"Training\")\n",
    "    class_distrib(y_val, labels_names, \"Validating\")\n",
    "    class_distrib(y_test, labels_names, \"Testing\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    val_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                   .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                      tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                   .batch(batch_size)\n",
    "                   .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                    .batch(batch_size)\n",
    "                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    print(f\"Training dataset:\\n {train_dataset}\")\n",
    "    for img, lbl in train_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nValidation dataset:\\n {val_dataset}\")\n",
    "    for img, lbl in val_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nTesting dataset:\\n {test_dataset}\")\n",
    "    for img, lbl in test_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    \n",
    "    \n",
    "    #### <<<<<<<<<<Pre-trained model>>>>>>>>>>\n",
    "    # Load ResNet50 pre-trained on ImageNet (w/out the top classification layer which is designed for ImageNet (diff dataset))\n",
    "    resnet_50_base = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Freeze the layers of VGG16 so they don't get updated during training - can unfreeze for fine tuning later\n",
    "    resnet_50_base.trainable = True\n",
    "    \n",
    "    # Add custom classification layers for CIFAR-100 (100 classes) - adapt model to CIFAR-100\n",
    "    model = models.Sequential([\n",
    "        resnet_50_base,\n",
    "        layers.GlobalAveragePooling2D(), # Better for ResNet than Flatten\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(100, activation='softmax')  # CIFAR-100 has 100 classes\n",
    "    ])\n",
    "    \n",
    "    for sample in test_dataset.take(1):\n",
    "        print(type(sample))  # Should be <class 'tuple'>\n",
    "        print(len(sample))  # Should be 2\n",
    "        print(type(sample[0]), type(sample[1]))  # Both should be <class 'tensorflow.Tensor'>\n",
    "        print(sample[0].shape)  # Should be (batch_size, 224, 224, 3)\n",
    "        print(sample[1].shape)  # Should be (batch_size, 100)\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    sample = next(iter(test_dataset.as_numpy_iterator()))\n",
    "    print(len(sample))  # Should be 2\n",
    "    print(type(sample[0]), type(sample[1]))  # Both should be <class 'numpy.ndarray'>\n",
    "    print(sample[0].shape, sample[1].shape)  # Should match model input and output\n",
    "    print(\"\\n\")\n",
    "    #for x, y in test_dataset.take(1):\n",
    "    #    print(type(x), type(y))  # Both should be <class 'tensorflow.Tensor'>\n",
    "    #for x_batch, y_batch in test_dataset.take(1):\n",
    "    #    test_loss, test_acc = model.evaluate(x_batch, y_batch)\n",
    "    #    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "    \n",
    "    # Compile the model\n",
    "    #tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.003),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', # or val_accuracy\n",
    "                                   patience=5, # Num. epochs with no improvements - help void overfitting\n",
    "                                   restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', # or val_accuracy\n",
    "    #                              factor=0.1, # Reduce lr by a factor\n",
    "    #                              patience=3, # Num epochs w/ no improvement\n",
    "    #                              min_lr=1e-6, # Min lr\n",
    "    #                              verbose=1)\n",
    "    #tensorboard = TensorBoard(log_dir='./logs', # Logs directory\n",
    "    #                         histogram_freq=1, # Logs histograms for weights/activations\n",
    "    #                         write_graph=True, # Logs graph of model\n",
    "    #                         write_images=True) # Log images like weight histogram\n",
    "    #checkpoint = ModelCheckpoint('best_model.h5',\n",
    "    #                             monitor='val_loss', # or val_accuracy\n",
    "    #                             save_best_only=True, # Save only best model\n",
    "    #                             mode='min', # min for loss or max for accuracy\n",
    "    #                             verbose=1)\n",
    "    #cvs_logger = CSVLogger('training_log.csv', seperator=',', append=True) # Save train metrics to analyse\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=25,\n",
    "                        batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history.history['loss'])+1)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    def plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc):\n",
    "        # Plot Training and Validation Loss\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_loss, label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_acc, label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    #tensorboard --logdir==path_to_your_logs\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e0be2-1f31-48bd-aeab-593292c987e9",
   "metadata": {},
   "source": [
    "# Increase model complexity () add layers ), reduce dropout and more epochs for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bb7ef-79a0-4397-bcbb-d863b8db4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat_2_times in range(2):\n",
    "    #### <<<<<<<<<<Load and process data>>>>>>>>>>\n",
    "    # Load CIFAR-100 dataset\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "    \n",
    "    # Split (8000) of training data into temporary set\n",
    "    X_temp, X_train, y_temp, y_train = train_test_split(X_train, y_train, test_size=0.84, stratify=y_train, random_state=42)\n",
    "    print(f\"X_temp.shape: {X_temp.shape}\\n\")\n",
    "    \n",
    "    # Split temp data into equal validation (4000) and testing (4000) data\n",
    "    X_temp_val, X_temp_test, y_temp_val, y_temp_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    print(f\"X_temp_val.shape: {X_temp_val.shape}\")\n",
    "    print(f\"y_temp_val.shape: {y_temp_val.shape}\")\n",
    "    print(f\"X_temp_test.shape: {X_temp_test.shape}\")\n",
    "    print(f\"y_temp_test.shape: {y_temp_test.shape}\\n\")\n",
    "    \n",
    "    # Split test data into validation (5000) and testing (5000)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "    \n",
    "    # Add temp_val to validation (9000) and temp_test to testing (9000) to get a 70/15/15 data split\n",
    "    X_val = np.concatenate((X_val, X_temp_val), axis=0)\n",
    "    y_val = np.concatenate((y_val, y_temp_val), axis=0)\n",
    "    X_test = np.concatenate((X_test, X_temp_test), axis=0)\n",
    "    y_test = np.concatenate((y_test, y_temp_test), axis=0)\n",
    "    \n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"y_train.shape: {y_train.shape}\")\n",
    "    print(f\"X_val.shape: {X_val.shape}\")\n",
    "    print(f\"y_val.shape: {y_val.shape}\")\n",
    "    print(f\"X_test.shape: {X_test.shape}\")\n",
    "    print(f\"y_test.shape: {y_test.shape}\\n\")\n",
    "    \n",
    "    def display_imgs(imgs, labels):\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            k = np.random.randint(0, imgs.shape[0])\n",
    "            if i == 0:\n",
    "                print(f\"labels[{k}].shape: {labels[k].shape}\")\n",
    "                print(f\"imgs[{k}].shape: {imgs[k].shape}\")\n",
    "            plt.imshow(imgs[k])\n",
    "            #plt.title(labels[k])\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    # Normalise images (scale to range [0, 1]) - Improves convergence speed & accuracy\n",
    "    X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    labels_names = ['beaver','dolphin','otter','seal','whale','aquarium fish','flatfish','ray','shark','trout',\n",
    "                   'orchids','poppies','roses','sunflowers','tulips','bottles','bowls','cans','cups','plates',\n",
    "                   'apples','mushrooms','oranges','pears','sweet peppers','clock','computer keyboard','lamp',\n",
    "                   'telephone','television','bed','chair','couch','table','wardrobe','bee','beetle','butterfly',\n",
    "                   'caterpillar','cockroach','bear','leopard','lion','tiger','wolf','bridge','castle','house',\n",
    "                   'road','skyscraper','cloud','forest','mountain','plain','sea','camel','cattle','chimpanzee',\n",
    "                   'elephant','kangaroo','fox','porcupine','possum','raccoon','skunk','crab','lobster','snail',\n",
    "                   'spider','worm','baby','boy','girl','man','woman','crocodile','dinosaur','lizard','snake',\n",
    "                   'turtle','hamster','mouse','rabbit','shrew','squirrel','maple','oak','palm','pine','willow',\n",
    "                   'bicycle','bus','motorcycle','pickup truck','train','lawn-mower','rocket','streetcar','tank',\n",
    "                   'tractor']\n",
    "    \n",
    "    def class_distrib(y, labels_names, dataset_name):\n",
    "        counts = pd.DataFrame(data=y).value_counts().sort_index()\n",
    "        #print(f\"counts:\\n{counts}\")\n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        ax.bar(labels_names, counts)\n",
    "        ax.set_xticklabels(labels_names, rotation=90, fontsize=15)\n",
    "        plt.title(f\"Distribution of '{dataset_name}' Dataset\", fontsize=25)\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    class_distrib(y_train, labels_names, \"Training\")\n",
    "    class_distrib(y_val, labels_names, \"Validating\")\n",
    "    class_distrib(y_test, labels_names, \"Testing\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    val_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                   .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                      tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                   .batch(batch_size)\n",
    "                   .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                    .batch(batch_size)\n",
    "                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    print(f\"Training dataset:\\n {train_dataset}\")\n",
    "    for img, lbl in train_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nValidation dataset:\\n {val_dataset}\")\n",
    "    for img, lbl in val_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nTesting dataset:\\n {test_dataset}\")\n",
    "    for img, lbl in test_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    \n",
    "    \n",
    "    #### <<<<<<<<<<Pre-trained model>>>>>>>>>>\n",
    "    # Load ResNet50 pre-trained on ImageNet (w/out the top classification layer which is designed for ImageNet (diff dataset))\n",
    "    resnet_50_base = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Freeze the layers of VGG16 so they don't get updated during training - can unfreeze for fine tuning later\n",
    "    resnet_50_base.trainable = False\n",
    "    \n",
    "    # Add custom classification layers for CIFAR-100 (100 classes) - adapt model to CIFAR-100\n",
    "    model = models.Sequential([\n",
    "        resnet_50_base,\n",
    "        layers.GlobalAveragePooling2D(), # Better for ResNet than Flatten\n",
    "        layers.Dense(1024, activation='relu'),  # Increase the number of units\n",
    "        layers.Dense(512, activation='relu'),  # Another hidden layer        \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(100, activation='softmax')  # CIFAR-100 has 100 classes\n",
    "    ])\n",
    "    \n",
    "    for sample in test_dataset.take(1):\n",
    "        print(type(sample))  # Should be <class 'tuple'>\n",
    "        print(len(sample))  # Should be 2\n",
    "        print(type(sample[0]), type(sample[1]))  # Both should be <class 'tensorflow.Tensor'>\n",
    "        print(sample[0].shape)  # Should be (batch_size, 224, 224, 3)\n",
    "        print(sample[1].shape)  # Should be (batch_size, 100)\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    sample = next(iter(test_dataset.as_numpy_iterator()))\n",
    "    print(len(sample))  # Should be 2\n",
    "    print(type(sample[0]), type(sample[1]))  # Both should be <class 'numpy.ndarray'>\n",
    "    print(sample[0].shape, sample[1].shape)  # Should match model input and output\n",
    "    print(\"\\n\")\n",
    "    #for x, y in test_dataset.take(1):\n",
    "    #    print(type(x), type(y))  # Both should be <class 'tensorflow.Tensor'>\n",
    "    #for x_batch, y_batch in test_dataset.take(1):\n",
    "    #    test_loss, test_acc = model.evaluate(x_batch, y_batch)\n",
    "    #    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "    \n",
    "    # Compile the model\n",
    "    #tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.003),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', # or val_accuracy\n",
    "    #                              factor=0.1, # Reduce lr by a factor\n",
    "    #                              patience=3, # Num epochs w/ no improvement\n",
    "    #                              min_lr=1e-6, # Min lr\n",
    "    #                              verbose=1)\n",
    "    #tensorboard = TensorBoard(log_dir='./logs', # Logs directory\n",
    "    #                         histogram_freq=1, # Logs histograms for weights/activations\n",
    "    #                         write_graph=True, # Logs graph of model\n",
    "    #                         write_images=True) # Log images like weight histogram\n",
    "    #checkpoint = ModelCheckpoint('best_model.h5',\n",
    "    #                             monitor='val_loss', # or val_accuracy\n",
    "    #                             save_best_only=True, # Save only best model\n",
    "    #                             mode='min', # min for loss or max for accuracy\n",
    "    #                             verbose=1)\n",
    "    #cvs_logger = CSVLogger('training_log.csv', seperator=',', append=True) # Save train metrics to analyse\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=40,\n",
    "                        batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history.history['loss'])+1)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    def plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc):\n",
    "        # Plot Training and Validation Loss\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_loss, label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_acc, label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    #tensorboard --logdir==path_to_your_logs\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist\n",
    "    \n",
    "    #### <<<<<<<<<<Fine-Tune>>>>>>>>>>\n",
    "    #### <<<<<<<<<<Adapt Model>>>>>>>>>>\n",
    "    # Unfreeze last 10 layers\n",
    "    for layer in resnet_50_base.layers[-10:]:\n",
    "        layer.trainable = True # Allow layers to be updated\n",
    "    \n",
    "    # Compile again w/ lower learning rate (prevents destroying learned features)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Modify Dataset>>>>>>>>>>\n",
    "    def augment_dataset(x, y):\n",
    "        x = tf.image.resize(x, (224, 224))  # Resize images\n",
    "        x = tf.image.random_flip_left_right(x)  # Random horizontal flip\n",
    "        x = tf.image.random_brightness(x, max_delta=0.2)  # Adjust brightness\n",
    "        x = tf.image.random_contrast(x, lower=0.8, upper=1.2)  # Adjust contrast\n",
    "        y = tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))  # One-hot encode labels\n",
    "        return x, y\n",
    "    train_dataset_aug = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(augment_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    # Not val or test as augment train helps generalise better, but want to provide consistent benchmark for eval perf\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Train the model\n",
    "    history_fine_tune = model.fit(train_dataset, validation_data=val_dataset, epochs=15,\n",
    "                                  batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history_fine_tune.history['loss'])+1)\n",
    "    train_loss = history_fine_tune.history['loss']\n",
    "    val_loss = history_fine_tune.history['val_loss']\n",
    "    train_acc = history_fine_tune.history['accuracy']\n",
    "    val_acc = history_fine_tune.history['val_accuracy']\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history_fine_tune.history)\n",
    "    hist['epoch'] = history_fine_tune.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ed4f2-f0e8-4e7d-a247-8617f6ccd5a1",
   "metadata": {},
   "source": [
    "# Different optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39409768-5573-4dee-af21-54ed7c3d1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat_2_times in range(2):\n",
    "    #### <<<<<<<<<<Load and process data>>>>>>>>>>\n",
    "    # Load CIFAR-100 dataset\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "    \n",
    "    # Split (8000) of training data into temporary set\n",
    "    X_temp, X_train, y_temp, y_train = train_test_split(X_train, y_train, test_size=0.84, stratify=y_train, random_state=42)\n",
    "    print(f\"X_temp.shape: {X_temp.shape}\\n\")\n",
    "    \n",
    "    # Split temp data into equal validation (4000) and testing (4000) data\n",
    "    X_temp_val, X_temp_test, y_temp_val, y_temp_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    print(f\"X_temp_val.shape: {X_temp_val.shape}\")\n",
    "    print(f\"y_temp_val.shape: {y_temp_val.shape}\")\n",
    "    print(f\"X_temp_test.shape: {X_temp_test.shape}\")\n",
    "    print(f\"y_temp_test.shape: {y_temp_test.shape}\\n\")\n",
    "    \n",
    "    # Split test data into validation (5000) and testing (5000)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "    \n",
    "    # Add temp_val to validation (9000) and temp_test to testing (9000) to get a 70/15/15 data split\n",
    "    X_val = np.concatenate((X_val, X_temp_val), axis=0)\n",
    "    y_val = np.concatenate((y_val, y_temp_val), axis=0)\n",
    "    X_test = np.concatenate((X_test, X_temp_test), axis=0)\n",
    "    y_test = np.concatenate((y_test, y_temp_test), axis=0)\n",
    "    \n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"y_train.shape: {y_train.shape}\")\n",
    "    print(f\"X_val.shape: {X_val.shape}\")\n",
    "    print(f\"y_val.shape: {y_val.shape}\")\n",
    "    print(f\"X_test.shape: {X_test.shape}\")\n",
    "    print(f\"y_test.shape: {y_test.shape}\\n\")\n",
    "    \n",
    "    def display_imgs(imgs, labels):\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            k = np.random.randint(0, imgs.shape[0])\n",
    "            if i == 0:\n",
    "                print(f\"labels[{k}].shape: {labels[k].shape}\")\n",
    "                print(f\"imgs[{k}].shape: {imgs[k].shape}\")\n",
    "            plt.imshow(imgs[k])\n",
    "            #plt.title(labels[k])\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    # Normalise images (scale to range [0, 1]) - Improves convergence speed & accuracy\n",
    "    X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
    "    display_imgs(X_train, y_train)\n",
    "    \n",
    "    labels_names = ['beaver','dolphin','otter','seal','whale','aquarium fish','flatfish','ray','shark','trout',\n",
    "                   'orchids','poppies','roses','sunflowers','tulips','bottles','bowls','cans','cups','plates',\n",
    "                   'apples','mushrooms','oranges','pears','sweet peppers','clock','computer keyboard','lamp',\n",
    "                   'telephone','television','bed','chair','couch','table','wardrobe','bee','beetle','butterfly',\n",
    "                   'caterpillar','cockroach','bear','leopard','lion','tiger','wolf','bridge','castle','house',\n",
    "                   'road','skyscraper','cloud','forest','mountain','plain','sea','camel','cattle','chimpanzee',\n",
    "                   'elephant','kangaroo','fox','porcupine','possum','raccoon','skunk','crab','lobster','snail',\n",
    "                   'spider','worm','baby','boy','girl','man','woman','crocodile','dinosaur','lizard','snake',\n",
    "                   'turtle','hamster','mouse','rabbit','shrew','squirrel','maple','oak','palm','pine','willow',\n",
    "                   'bicycle','bus','motorcycle','pickup truck','train','lawn-mower','rocket','streetcar','tank',\n",
    "                   'tractor']\n",
    "    \n",
    "    def class_distrib(y, labels_names, dataset_name):\n",
    "        counts = pd.DataFrame(data=y).value_counts().sort_index()\n",
    "        #print(f\"counts:\\n{counts}\")\n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        ax.bar(labels_names, counts)\n",
    "        ax.set_xticklabels(labels_names, rotation=90, fontsize=15)\n",
    "        plt.title(f\"Distribution of '{dataset_name}' Dataset\", fontsize=25)\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    class_distrib(y_train, labels_names, \"Training\")\n",
    "    class_distrib(y_val, labels_names, \"Validating\")\n",
    "    class_distrib(y_test, labels_names, \"Testing\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    val_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "                   .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                      tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                   .batch(batch_size)\n",
    "                   .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "                     .map(lambda x, y: (tf.image.resize(x, (224, 224)),\n",
    "                                        tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))))  # Remove extra dimension\n",
    "                    .batch(batch_size)\n",
    "                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    print(f\"Training dataset:\\n {train_dataset}\")\n",
    "    for img, lbl in train_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nValidation dataset:\\n {val_dataset}\")\n",
    "    for img, lbl in val_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    print(f\"\\nTesting dataset:\\n {test_dataset}\")\n",
    "    for img, lbl in test_dataset.take(1):\n",
    "        #if isinstance(batch, tuple) and len(batch) == 2:\n",
    "        print(f\"Image shape: {img.shape}\")  # Should be (batch_size, 224, 224, 3)\n",
    "        print(f\"Label shape: {lbl.shape}\")  # Should be (batch_size, 10)\n",
    "        del img,lbl\n",
    "    \n",
    "    \n",
    "    #### <<<<<<<<<<Pre-trained model>>>>>>>>>>\n",
    "    # Load ResNet50 pre-trained on ImageNet (w/out the top classification layer which is designed for ImageNet (diff dataset))\n",
    "    resnet_50_base = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Freeze the layers of VGG16 so they don't get updated during training - can unfreeze for fine tuning later\n",
    "    resnet_50_base.trainable = False\n",
    "    \n",
    "    # Add custom classification layers for CIFAR-100 (100 classes) - adapt model to CIFAR-100\n",
    "    model = models.Sequential([\n",
    "        resnet_50_base,\n",
    "        layers.GlobalAveragePooling2D(), # Better for ResNet than Flatten\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(100, activation='softmax')  # CIFAR-100 has 100 classes\n",
    "    ])\n",
    "    \n",
    "    for sample in test_dataset.take(1):\n",
    "        print(type(sample))  # Should be <class 'tuple'>\n",
    "        print(len(sample))  # Should be 2\n",
    "        print(type(sample[0]), type(sample[1]))  # Both should be <class 'tensorflow.Tensor'>\n",
    "        print(sample[0].shape)  # Should be (batch_size, 224, 224, 3)\n",
    "        print(sample[1].shape)  # Should be (batch_size, 100)\n",
    "    print(f\"Model input shape: {model.input_shape}\")\n",
    "    print(f\"Model output shape: {model.output_shape}\")\n",
    "    sample = next(iter(test_dataset.as_numpy_iterator()))\n",
    "    print(len(sample))  # Should be 2\n",
    "    print(type(sample[0]), type(sample[1]))  # Both should be <class 'numpy.ndarray'>\n",
    "    print(sample[0].shape, sample[1].shape)  # Should match model input and output\n",
    "    print(\"\\n\")\n",
    "    #for x, y in test_dataset.take(1):\n",
    "    #    print(type(x), type(y))  # Both should be <class 'tensorflow.Tensor'>\n",
    "    #for x_batch, y_batch in test_dataset.take(1):\n",
    "    #    test_loss, test_acc = model.evaluate(x_batch, y_batch)\n",
    "    #    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "    \n",
    "    # Compile the model\n",
    "    #tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    model.compile(optimizer=optimizers.RMSProp(learning_rate=1e-3, momenutm=0.1),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', # or val_accuracy\n",
    "                                   patience=5, # Num. epochs with no improvements - help void overfitting\n",
    "                                   restore_best_weights=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', # or val_accuracy\n",
    "    #                              factor=0.1, # Reduce lr by a factor\n",
    "    #                              patience=3, # Num epochs w/ no improvement\n",
    "    #                              min_lr=1e-6, # Min lr\n",
    "    #                              verbose=1)\n",
    "    #tensorboard = TensorBoard(log_dir='./logs', # Logs directory\n",
    "    #                         histogram_freq=1, # Logs histograms for weights/activations\n",
    "    #                         write_graph=True, # Logs graph of model\n",
    "    #                         write_images=True) # Log images like weight histogram\n",
    "    #checkpoint = ModelCheckpoint('best_model.h5',\n",
    "    #                             monitor='val_loss', # or val_accuracy\n",
    "    #                             save_best_only=True, # Save only best model\n",
    "    #                             mode='min', # min for loss or max for accuracy\n",
    "    #                             verbose=1)\n",
    "    #cvs_logger = CSVLogger('training_log.csv', seperator=',', append=True) # Save train metrics to analyse\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=25,\n",
    "                        batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history.history['loss'])+1)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    def plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc):\n",
    "        # Plot Training and Validation Loss\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_loss, label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_acc, label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    #tensorboard --logdir==path_to_your_logs\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist\n",
    "    \n",
    "    #### <<<<<<<<<<Fine-Tune>>>>>>>>>>\n",
    "    #### <<<<<<<<<<Adapt Model>>>>>>>>>>\n",
    "    # Unfreeze last 10 layers\n",
    "    for layer in resnet_50_base.layers[-10:]:\n",
    "        layer.trainable = True # Allow layers to be updated\n",
    "    \n",
    "    # Compile again w/ lower learning rate (prevents destroying learned features)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'f1_score'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #### <<<<<<<<<<Modify Dataset>>>>>>>>>>\n",
    "    def augment_dataset(x, y):\n",
    "        x = tf.image.resize(x, (128, 128))  # Resize images\n",
    "        x = tf.image.random_flip_left_right(x)  # Random horizontal flip\n",
    "        x = tf.image.random_brightness(x, max_delta=0.2)  # Adjust brightness\n",
    "        x = tf.image.random_contrast(x, lower=0.8, upper=1.2)  # Adjust contrast\n",
    "        y = tf.squeeze(tf.one_hot(y, depth=100, dtype=tf.float32))  # One-hot encode labels\n",
    "        return x, y\n",
    "    train_dataset_aug = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                     .map(augment_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                     .batch(batch_size)\n",
    "                     .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    # Not val or test as augment train helps generalise better, but want to provide consistent benchmark for eval perf\n",
    "    \n",
    "    #### <<<<<<<<<<Train Model & Track Training/Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Train the model\n",
    "    history_fine_tune = model.fit(train_dataset, validation_data=val_dataset, epochs=15,\n",
    "                                  batch_size=batch_size, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    #### <<<<<<<<<<Plot Training & Validation Error>>>>>>>>>>\n",
    "    \n",
    "    # Extract loss and accuracy\n",
    "    epochs = range(1,len(history_fine_tune.history['loss'])+1)\n",
    "    train_loss = history_fine_tune.history['loss']\n",
    "    val_loss = history_fine_tune.history['val_loss']\n",
    "    train_acc = history_fine_tune.history['accuracy']\n",
    "    val_acc = history_fine_tune.history['val_accuracy']\n",
    "    \n",
    "    plot_evidence(epochs, train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    #### <<<<<<<<<<Evaluate Model on Test Data>>>>>>>>>>\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    results = model.evaluate(test_dataset)\n",
    "    test_loss = results[0]\n",
    "    test_acc = results[1]\n",
    "    test_precision = results[2]\n",
    "    test_f1_scores = results[3]\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss*100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Test F1 Scores (Per Class): {test_f1_scores.numpy()*100}\")\n",
    "    print(f\"Average Test F1 Scores:{np.average(test_f1_scores.numpy()*100):.2f}\\n\")\n",
    "    \n",
    "    #### <<<<<<<<<<Generate Confusion Matrix>>>>>>>>>>\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test_revised = tf.image.resize(X_test, (224, 224))\n",
    "    y_pred = model.predict(X_test_revised)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix) #cmap='Blues', fmt='d'\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=labels_names))\n",
    "    \n",
    "    # Create a DataFrame from the history of the training and store the epoch values.\n",
    "    hist = pd.DataFrame(history_fine_tune.history)\n",
    "    hist['epoch'] = history_fine_tune.epoch\n",
    "    \n",
    "    # Finally, display the hist DataFrame.\n",
    "    hist"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
